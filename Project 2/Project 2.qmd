---
title: "Project 2"
author: "Group 5: Katelyn Wnuk, Brennan Chan, Emmett Collins"
format: pdf
editor: visual
---

Load in the data keeping only numeric values for an easier pipeline. 
```{r, message=FALSE, warning=FALSE}
library(tidymodels)
library(tidyverse)
library(car)
library(broom)
library(olsrr)
library(dplyr)
library(GGally)
set.seed(123) 

data_car = read_csv("http://www.jpstats.org/data/car_price.csv") |>
  select( where(~ !is.character(.x)) )
```

# Model Selection

To identify an appropriate regression model, we evaluated all possible subsets of predictors and compared them using adjusted $R^2$, which balances model fit against unnecessary complexity. Adjusted $R^2$ increases only when a predictor provides meaningful explanatory value beyond what is already included. The all-subsets procedure identified a model with 14 predictors that achieved the highest adjusted $R^2$. This model was selected for further analysis, including outlier detection, assumption checking, and validation.
```{r}
fit = lm(price ~ ., data = data_car)
subsets_best = ols_step_best_subset(fit) 
plot(subsets_best)
subsets_best
```

Several candidate regression models were evaluated using adjusted as the primary selection criterion. Adjusted is preferred over the unadjusted statistic because it penalizes unnecessary model complexity. A predictor will only increase adjusted if it provides meaningful explanatory power beyond what is already captured by the existing variables. Thus, adjusted $R^2$ offers a more objective balance between goodness of fit and the risk of overfitting.

Among all fitted models, the specification containing 14 predictor variables ( car_ID, symboling, wheelbase, carlength, carwidth, carheight, curbweight, enginesize, stroke, compressionratio, horsepower, peakrpm, citympg, highwaympg ) produced the largest adjusted $R^2$ This indicates that, after accounting for the number of predictors included, this model explains the greatest proportion of variation in car prices among the considered alternatives. Therefore, this model was selected for further analysis, including outlier detection, assumption checking, and model validation.

## Fitting the model

The dataset was split into an 80% training set and a 20% testing set to allow for later validation. Using the training data, a linear regression model was fit with the 14 selected predictors. The model was constructed through a tidymodels workflow combining the recipe and the OLS engine.

```{r}
data_split <- initial_split(data_car, prop = 0.80)
train_data <- training(data_split)
test_data  <- testing(data_split)

car_recipe <- recipe(price ~ 
                       car_ID + symboling + wheelbase + carlength + 
                       carwidth + carheight + curbweight + enginesize +
                       stroke + compressionratio + horsepower + peakrpm +
                       citympg + highwaympg,
                     data = train_data)

car_model <- linear_reg() |>
  set_engine("lm")

car_workflow <- workflow() |>
  add_recipe(car_recipe) |>
  add_model(car_model)

fitted_model <- car_workflow |>
  fit(data <- train_data)

fitted_model |> glance()
```

# Outlier Detection

To identify potential outliers and influential points, we examined several diagnostic plots and statistics from the fitted model. The residuals vs. leverage plot and Cook's distance bar plot were generated to assess the influence of individual observations on the regression results.

```{r}
fitted_model |> extract_fit_engine() |> ols_plot_resid_lev()
```

we can see from the residuals vs leverage plot that there are 10 points (6, 130, 147, 83, 71, 8, 106, 28, 158, 110) that stand out as potentially influential outliers. To further investigate these points, we can look at the Cook's distance bar plot and DFBETAS plot.

```{r}
fitted_model |> extract_fit_engine() |> ols_plot_cooksd_bar()
```

we can see from the cooksd plot that many of the outliers are influential to the model. Finally, we can look at the DFBETAS plot to see how much each point is influencing the coefficients of the model.

```{r}
fitted_model |> extract_fit_engine() |> ols_plot_dfbetas()
```

From the DFBETAS plot, we can see that there are several points that are influencing the coefficients of the model significantly. Based on these diagnostic plots, we identified a total of 15 points that were marked as non-normal. These points will be further investigated to determine if they should be removed from the dataset or if they are valid observations that should be kept.

## Testing to see if removing outliers helps the model

Influential observations were identified using the residuals–leverage plot, Cook’s distance, and DFBETAs. These diagnostics revealed several points with both high leverage and large residuals, indicating unusually strong influence on the fitted model. Observations 6, 130, 147, 83, 71, 8, 106, 28, 158, and 110 were consistently flagged across multiple measures. Because such points can distort coefficient estimates and inflate error variability, they were investigated further and removed for a refined model fit.
```{r}
extreme_outliers <- c(6, 130, 147, 83, 71, 8, 106, 28, 158, 110)

clean_train <- train_data[-extreme_outliers, ]

refit_model <- car_workflow |>
  fit(data = clean_train)

original_summary <- fitted_model |> glance()
cleaned_summary  <- refit_model   |> glance()

original_summary
cleaned_summary
```
A new model was fit after removing the ten most influential observations from the training set to assess their effect on model performance. The refined model showed clear improvement, with increased adjusted $R^2$, reduced residual standard error, and lower AIC/BIC values. These changes indicate a more stable and precise relationship between predictors and car price. Based on this improvement, the refined model was used for assumption checking and validation.

## Checking Assumptions for the Multiple Regression Model

### 1. Linearity & Constant Variance (Homoscedasticity)
```{r}
lm_refined <- extract_fit_engine(refit_model)
ols_plot_resid_fit(lm_refined)
```
The residuals appear randomly scattered around zero with no systematic curvature, which indicates that the linearity assumption is reasonable. The vertical spread of the residuals does not show a strong funnel shape, suggesting that the constant variance assumption is not severely violated.

### 2. Normality of the Error Terms
```{r}
ols_plot_resid_hist(lm_refined)
ols_plot_resid_qq(lm_refined)
```
The histogram is approximately bell-shaped, and the Q–Q plot shows points lying close to the reference line with only mild deviations in the tails. This suggests that the normality assumption is reasonably satisfied.

### 3. Independence of Errors
```{r}
durbinWatsonTest(lm_refined)
```
The Durbin–Watson test produced a p-value greater than 0.05, providing no evidence of autocorrelation. Thus, the independence assumption appears to hold.

### 4. Multicollinearity
```{r}
vif(lm_refined)
```
Several predictors exhibit large VIF values, indicating multicollinearity. As emphasized in the notes, multicollinearity does not violate the assumptions of the Multiple Regression Model but can make individual coefficient estimates unstable. Because the goal of this project is prediction rather than inference, no corrective action was taken.


## Model Validation

### 1. Generate Test-Set Predictions
```{r}
test_results <- predict(refit_model, new_data = test_data) |>
bind_cols(test_data)
```
Using the cleaned model (with extreme outliers removed), we generate predicted prices for the 20% testing subset. These predictions allow us to evaluate out-of-sample performance, exactly as described in Section 22.2 of the notes. The test set has not influenced model training, so it provides an unbiased measure of predictive accuracy.

### 2. Compute Validation Metrics
```{r}
validation_metrics <- test_results |>
metrics(truth = price, estimate = .pred)

validation_metrics
```
The test-set metrics indicate that the refined model provides reasonably strong predictive performance. The RMSE of around $3569$ and MAE of around $2574$ show that typical prediction errors are moderate relative to the overall price range, and the MAE being smaller than the RMSE suggests only a few larger residuals. The test-set $R^2$ of 0.789 means the model explains roughly 79% of the variability in car prices for unseen data, which aligns well with its training performance. Overall, the model generalizes well and does not show signs of substantial overfitting.


### 3. Plot Predicted vs. Actual Prices
```{r}
ggplot(test_results, aes(x = price, y = .pred)) +
geom_point(color = "blue") +
geom_abline(color = "red") +
labs(
title = "Model Validation: Actual vs Predicted Prices",
x = "Actual Price",
y = "Predicted Price"
) +
theme_minimal()

```
The validation plot mirrors the example in the course notes: a 45-degree reference line is used to visually assess predictive alignment. If predictions closely follow this line, the model is accurately capturing the relationship in the test data. Here, the points show a reasonably tight linear pattern without major systematic deviations, indicating consistent predictive performance.
